---
title: "About"
updated: "2025-11-09"
---

I'm finishing my PhD at Ohio State, where I focus on selective prediction and uncertainty quantification. My work addresses a fundamental question: how do we build ML systems that know when they don't know something — and admit it gracefully?

Most ML systems are trained to always give an answer, even when they shouldn't. They'll confidently predict on out-of-distribution data, hallucinate facts, or make decisions they're not equipped to handle. This gap between confidence and competence is particularly dangerous in production settings where bad predictions have real consequences. My research directly addresses a core challenge in AI alignment: building systems that are honest about their uncertainty rather than confidently hallucinating.

My research centers on building systems that can recognize their own uncertainty and abstain from predictions when appropriate. The core idea is simple: a model that says "I don't know" and routes to a human is more reliable than one that guesses wrong with high confidence. This matters especially in domains like defense, medical imaging, and content moderation — anywhere the cost of a mistake outweighs the cost of asking for help.

The challenge isn't just technical. It's about changing how we think about ML systems — from tools that must always have an answer to tools that know their limits. That means developing better calibration methods, designing human-in-the-loop workflows that actually work in practice, and building evaluation frameworks that measure not just accuracy but reliability.

Day to day, I write Python and PyTorch, run experiments on HPC clusters, and build the plumbing that makes research usable — data pipelines, benchmarks, reproducible code.

My latest work pushes this further into large language models. Rather than training a single "I don't know" behavior, I'm exploring how models can learn to distinguish between different kinds of uncertainty — questions they could answer with more reasoning, questions that need external context, and questions that are genuinely beyond reach. The goal is structured, calibrated behavior: models that don't just abstain, but diagnose why they're uncertain and route to the right next step. This connects directly to the core challenge of building AI systems we can actually trust in deployment.

Looking for research scientist or ML engineering roles in 2026. U.S. citizen, comfortable with CUI/DoD work.
