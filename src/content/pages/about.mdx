---
title: "About"
updated: "2025-11-09"
---

I'm finishing my PhD at Ohio State, where I focus on selective prediction and uncertainty quantification. My work addresses a fundamental question: how do we build ML systems that know when they don't know something — and admit it gracefully?

Most ML systems are trained to always give an answer, even when they shouldn't. They'll confidently predict on out-of-distribution data, hallucinate facts, or make decisions they're not equipped to handle. This gap between confidence and competence is particularly dangerous in production settings where bad predictions have real consequences. My research directly addresses a core challenge in AI alignment: building systems that are honest about their uncertainty rather than confidently hallucinating.

My research centers on building systems that can recognize their own uncertainty and abstain from predictions when appropriate. The core idea is simple: a model that says "I don't know" and routes to a human is more reliable than one that guesses wrong with high confidence. This matters especially in domains like defense, medical imaging, and content moderation — anywhere the cost of a mistake outweighs the cost of asking for help.

The challenge isn't just technical. It's about changing how we think about ML systems — from tools that must always have an answer to tools that know their limits. That means developing better calibration methods, designing human-in-the-loop workflows that actually work in practice, and building evaluation frameworks that measure not just accuracy but reliability.

Day to day, I write Python and PyTorch, run experiments on HPC clusters, and build the plumbing that makes research usable — data pipelines, benchmarks, reproducible code.

Looking for research scientist or ML engineering roles in 2026. U.S. citizen, comfortable with CUI/DoD work.
